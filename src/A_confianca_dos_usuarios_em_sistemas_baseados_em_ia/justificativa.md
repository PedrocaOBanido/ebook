# Justificativa

O avanço da Inteligência Artificial (IA) levanta preocupações sobre privacidade, vieses e perda de controle humano, especialmente com a popularização de modelos generativos, o que afeta diretamente a confiança dos usuários (Sargent et al., 2024). A confiança é um elemento central para a adoção responsável da IA e é influenciada por uma combinação de fatores técnicos, socioéticos e características individuais dos usuários, indo além do mero desempenho do sistema (BACH et al., 2024).

A natureza de "caixa-preta" de muitos sistemas de IA compromete a capacidade dos usuários de compreender e avaliar seus resultados (BACH et al., 2024; LIAO; SUNDAR, 2022). Por isso, é crucial que o design dos sistemas de IA inclua "pistas de confiabilidade" para ajudar os usuários a formar julgamentos adequados, evitando tanto a confiança excessiva quanto a desconfiança infundada (LIAO; SUNDAR, 2022). A adoção de princípios éticos explícitos, como transparência, justiça e responsabilidade, é fundamental, conforme reforçam a UNESCO (2021) e estudos que mostram a valorização pública desses múltiplos critérios éticos (KIESLICH; KELLER; STARKE, 2022).

Este estudo foca em usuários não especialistas, que interagem com a IA no dia a dia e, por não terem conhecimento técnico aprofundado, dependem de sinais indiretos para formar sua confiança (BACH et al., 2024; LIAO; SUNDAR, 2022). A construção de sistemas confiáveis exige uma abordagem que abranja todo o ciclo de vida da tecnologia, desde a coleta de dados até o monitoramento contínuo, integrando robustez, explicabilidade, justiça e privacidade em todas as fases (Li et al., 2022).

Como destacam Li et al. (2022), uma falha em qualquer etapa pode comprometer a confiança no sistema inteiro, o que reforça a importância de práticas interdisciplinares e avaliação contínua. Portanto, investigar a percepção desses usuários é crucial para desenvolver um design de IA ético, centrado no humano e alinhado às expectativas da sociedade.

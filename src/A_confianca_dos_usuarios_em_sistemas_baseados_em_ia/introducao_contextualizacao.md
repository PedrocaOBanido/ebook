# Introdução e Contextualização Teórica

A Inteligência Artificial (IA) tornou-se uma força transformadora no cotidiano, presente em setores como saúde, transporte e educação. Originada na década de 1950, a IA evoluiu de sistemas básicos para complexos assistentes e agentes autônomos que influenciam diretamente a agência humana e a tomada de decisões (AFROOGH et al., 2024). Embora promova eficiência e inovação, essa expansão tecnológica levanta novas e significativas questões éticas e sociais.

Apesar de seus benefícios, a aceitação da IA é dificultada pela desconfiança dos usuários. Preocupações com privacidade, discriminação algorítmica e a perda de autonomia humana afetam negativamente a confiança (SARGENT et al., 2024). O uso indevido de dados, a falta de transparência em decisões automatizadas e a disseminação de vieses reforçam essa desconfiança generalizada (BACH et al., 2024). Um relatório da UNESCO também alerta que, sem uma governança ética, a IA pode aprofundar desigualdades e ameaçar direitos humanos fundamentais (UNESCO, 2021).

A complexidade e a opacidade dos sistemas de IA, juntamente com sua capacidade de aprendizado autônomo, dificultam a previsão e a compreensão de seu comportamento, agravando a desconfiança (BACH et al., 2024). A confiança em sistemas de IA depende da correta interpretação de sinais como explicações claras e interações transparentes. A falta desses elementos pode levar a julgamentos equivocados e ao uso inadequado da tecnologia (LIAO; SUNDAR, 2022).

Para construir confiança, é essencial adotar um design ético e centrado no ser humano, incorporando princípios como transparência, justiça, responsabilidade e explicabilidade (KIESLICH; KELLER; STARKE, 2022). A percepção pública apoia a aplicação equilibrada desses princípios, mesmo que existam compromissos técnicos, como o trade-off entre explicabilidade e precisão. Iniciativas globais, como a Recomendação da UNESCO (2021), reforçam a necessidade de um desenvolvimento tecnológico que respeite a dignidade humana e a equidade social.